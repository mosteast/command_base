#!/usr/bin/env node

"use strict";

const path = require("path");
const yargs_factory = require("yargs/yargs");

const {
  default_ai_platform,
  supported_ai_platforms,
  ensure_prompt_source,
  expand_patterns,
  file_has_content,
  run_ai_command,
} = require("../utility/_ai_cli_utils");

(async () => {
  const { default: chalk } = await import("chalk");

  const command_name = path.basename(process.argv[1] || "text_summarize");
  const terminal_width = Math.min(120, process.stdout.columns || 120);

  const argv = yargs_factory(process.argv.slice(2))
    .scriptName(command_name)
    .usage(
      "Usage: $0 [options] <file|glob ...>" +
        "\n\nDescription:" +
        "\n  Summarize one or more files with the summarize_text prompt and write `[filename].summarized.md` next to each input file.",
    )
    .wrap(terminal_width)
    .option("refresh", {
      alias: "r",
      type: "boolean",
      default: false,
      describe:
        "Regenerate summaries even when the output file already exists.",
    })
    .option("ai-platform", {
      type: "string",
      default: default_ai_platform,
      choices: supported_ai_platforms,
      describe: "AI platform adapter to use (defaults to project preference).",
    })
    .option("ai-model", {
      type: "string",
      describe: "Override the default model for the chosen AI platform.",
    })
    .option("ai-temperature", {
      type: "number",
      describe:
        "Sampling temperature for the AI request (falls back to prompt defaults).",
    })
    .option("ai-max-tokens", {
      type: "number",
      describe:
        "Maximum tokens for the AI response (falls back to prompt defaults).",
    })
    .example(
      "$0 notes/**/*.md",
      "Summarize every Markdown file inside the notes directory tree.",
    )
    .example(
      "$0 report.md --refresh --ai-platform openai",
      "Regenerate the summary for report.md with the OpenAI adapter even if it already exists.",
    )
    .alias("h", "help")
    .demandCommand(1, "Provide at least one file path or glob pattern.")
    .epilog(
      "Options: --refresh, --ai-platform, --ai-model, --ai-temperature, --ai-max-tokens." +
        "\nExamples: text_summarize report.md | text_summarize docs/**/*.md",
    ).argv;

  const repo_root_path = path.resolve(__dirname, "..");
  const prompt_source_path =
    "/Users/hailang/Library/Mobile Documents/com~apple~CloudDocs/main/download/tool/prompt/summarize_text.build.md";

  const logger = create_logger(chalk, command_name);

  const prompt_file_path = await ensure_prompt_source("summarize_text", {
    repo_root: repo_root_path,
    external_path: prompt_source_path,
    logger,
  });

  const resolved_input_files = expand_patterns(argv._, { cwd: process.cwd() });

  if (resolved_input_files.length === 0) {
    logger.warn("No files matched the provided patterns.");
    return;
  }

  logger.info(
    `Preparing summaries for ${resolved_input_files.length} file${
      resolved_input_files.length === 1 ? "" : "s"
    }...`,
  );

  let processed_count = 0;
  let skipped_count = 0;
  let failed_count = 0;

  for (const absolute_input_path of resolved_input_files) {
    const relative_input_path =
      path.relative(process.cwd(), absolute_input_path) ||
      path.basename(absolute_input_path);
    const parsed_path = path.parse(absolute_input_path);
    const output_file_path = path.join(
      parsed_path.dir,
      `${parsed_path.name}.summarized.md`,
    );

    if (!argv.refresh && (await file_has_content(output_file_path))) {
      skipped_count += 1;
      console.log(
        chalk.gray(`Skipping existing summary: ${relative_input_path}`),
      );
      continue;
    }

    console.log(chalk.cyan(`Summarizing: ${relative_input_path}`));

    try {
      await run_ai_command({
        prompt_file: prompt_file_path,
        input_file: absolute_input_path,
        output_file: output_file_path,
        repo_root: repo_root_path,
        platform: argv["ai-platform"],
        model: argv["ai-model"],
        temperature: argv["ai-temperature"],
        max_tokens: argv["ai-max-tokens"],
        logger,
      });

      console.log(
        chalk.green(
          `Saved: ${
            path.relative(process.cwd(), output_file_path) ||
            path.basename(output_file_path)
          }`,
        ),
      );
      processed_count += 1;
    } catch (error) {
      failed_count += 1;
      console.error(chalk.red(`Failed: ${relative_input_path}`));
      console.error(chalk.red(error.message || error));
    }
  }

  const summary_parts = [];
  if (processed_count > 0) {
    summary_parts.push(chalk.green(`${processed_count} summarized`));
  }
  if (skipped_count > 0) {
    summary_parts.push(chalk.gray(`${skipped_count} skipped`));
  }
  if (failed_count > 0) {
    summary_parts.push(chalk.red(`${failed_count} failed`));
  }
  const summary_tail =
    summary_parts.length > 0 ? summary_parts.join(", ") : "no actions";
  console.log(chalk.blue(`Done (${summary_tail}).`));
})().catch((error) => {
  console.error(error);
  process.exitCode = 1;
});

function create_logger(chalk, command_name) {
  const prefix = chalk.magenta(`[${command_name}]`);
  return {
    info: (message) => console.log(prefix, chalk.cyan(message)),
    warn: (message) => console.warn(prefix, chalk.yellow(message)),
    error: (message) => console.error(prefix, chalk.red(message)),
  };
}
